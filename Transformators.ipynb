{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uqZ8civlTKV"
   },
   "source": [
    "# Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Destription:\n",
    "\n",
    "    Cointains some classes and functions for text cleaning and transformation.\n",
    "    Meant for use as transformers in pipelines.\n",
    "\n",
    "Funtions:\n",
    "\n",
    "    lemmatize - Lemmatizer\n",
    "    stemming - Porter Stemming \n",
    "    \n",
    "Classes:\n",
    "\n",
    "    CleanText - some basic text cleaning methods\n",
    "    Embeddings - build embeddings, tranform text data into numeric representation\n",
    "    FixLength - standardize length of the sentences (text input)\n",
    "    Flatten - flatten the output\n",
    "    Tag - finds part of the speech for every word in sentence\n",
    "    SentiFeatures - Calculates sentiment for every word in sentence using sentiwordnet\n",
    "    CustomBinarizer - wraper for LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "vqdjZO8YlTKo"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funtions:\n",
    "    lemmatize - Lemmatizer\n",
    "    stemming - Porter Stemming \n",
    "    \n",
    "Class:\n",
    "    CleanText - some basic text cleaning methods\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def _penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize( input_text, lemmatize_apply = 1):\n",
    "    \"\"\"\n",
    "    Lemmatizer function\n",
    "    input_text - text to lemmatize (sentence)\n",
    "    \"\"\"\n",
    "    lemmatizer =  WordNetLemmatizer()\n",
    "    input_text = word_tokenize(input_text)\n",
    "    tagged_sentence = pos_tag(input_text)\n",
    "    \n",
    "    lemmatized_text = []\n",
    "    for word, tag in tagged_sentence:       \n",
    "        if lemmatize_apply == True:\n",
    "            wn_tag = _penn_to_wn(tag)\n",
    "            if not wn_tag:\n",
    "                lemmatized_text.append(word)\n",
    "            else:\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                lemmatized_text.append(lemma)\n",
    "    return lemmatized_text\n",
    "    \n",
    "def stemming(input_text):\n",
    "    \"\"\"\n",
    "    Porter Stemming \n",
    "    input_text - text to stem (sentence)\n",
    "    \"\"\"\n",
    "    porter = PorterStemmer()\n",
    "    words = word_tokenize(input_text) \n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Some basic text cleaning methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stemming_apply = True, tokenize_apply = True, stopwords_apply = True):\n",
    "        self.stemming_apply = stemming_apply\n",
    "        self.tokenize_apply = tokenize_apply\n",
    "        self.stopwords_apply = stopwords_apply\n",
    "    \n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = '\"#$%&\\'()*+,-/:;<=>[\\\\]^_`{|}~'  #string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_noisy_signs(self, input_text):\n",
    "        return re.sub(r'[^\\w !?.]', '', input_text)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        if self.stopwords_apply == True:\n",
    "            stopwords_list = stopwords.words('english')\n",
    "            # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "            whitelist = [\"n't\", \"not\", \"no\"]\n",
    "            words = input_text.split() \n",
    "            clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "            return \" \".join(clean_words) \n",
    "        else:\n",
    "            return input_text\n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        if self.stemming_apply == True:\n",
    "            porter = PorterStemmer()\n",
    "            words = input_text.split() \n",
    "            stemmed_words = [porter.stem(word) for word in words]\n",
    "            return \" \".join(stemmed_words)\n",
    "        else:\n",
    "            return input_text\n",
    "    \n",
    "    def tokenize(self, input_text):\n",
    "        if self.tokenize_apply:\n",
    "            return word_tokenize(input_text)\n",
    "        else:\n",
    "            return input_text\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming).apply(self.tokenize)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nYPYCw6lTKx"
   },
   "source": [
    "## Text transformation: Embeddings, FixLength, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IHCsbqdllTKy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classes:\n",
    "    Embeddings - build embeddings, tranform text data into numeric representation\n",
    "    FixLength - standardize length of the sentences (text input)\n",
    "    Flatten - flatten the output\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import gensim\n",
    "\n",
    "\n",
    "class embeddings(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Class to build embeddings, and tranform text data into numeric representation\n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(self, embeddings = None, emb_source = 'load', emb_type = 'each', emb_size = None):\n",
    "        \"\"\"\n",
    "        build embeddings: own, or load from gensim, or build using gensim package\n",
    "        embeddings - if we have already loaded embedding we can send it in init, \n",
    "            if there is None, depending on setting parameter emb_source embedding is load from\n",
    "            gensim package or build using gensim package based on dataset in fit method\n",
    "        emb_source - if paremeter 'embeddings' is not specified (=None) there is need to load embedding\n",
    "            (set value to 'load') or build own embedding based on input dataset from fit method (value 'own')\n",
    "        emb_type - method of transforming text date into numeric representation (embeddings) \n",
    "        value 'each' stand for making embedding representation for each word in sentence (requires equal sentence length)\n",
    "        value 'avg' gives average embedding representation from all words in sentence\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.emb_source = emb_source\n",
    "        self.emb_type = emb_type\n",
    "        self.emb_size = emb_size if embeddings is None else embeddings.vector_size\n",
    " \n",
    "    def _build_embeddings_own(self, X_train, emb_size):\n",
    "        \n",
    "        if type(emb_size) == type(None):\n",
    "            raise ValueError('Pass parameter emb_size in fit function (size/dim of embedding)')\n",
    "            \n",
    "        if type(X_train) == type(None):\n",
    "            raise ValueError('Pass parameter X_train in fit function (dataset for training embeddings)')\n",
    "        \n",
    "        if self.embeddings == None:\n",
    "            own_embeddings = gensim.models.Word2Vec(X_train\n",
    "                     , min_count=1\n",
    "                     , size = emb_size\n",
    "                     , window=5\n",
    "                     , workers=8\n",
    "                     , sg = 1\n",
    "#                      , hs = 0\n",
    "#                      , negative = 10\n",
    "                     , seed = 0)\n",
    "        else:\n",
    "            own_embeddings = self.embeddings\n",
    "        own_embeddings.train(X_train, total_examples = own_embeddings.corpus_count, epochs = 20)\n",
    "        self.source = 'own'\n",
    "        self.emb_size = emb_size\n",
    "        self.embeddings = own_embeddings\n",
    "        \n",
    "    def _build_embeddings_gensim_load(self, gensim_file_path,\n",
    "                                     vocabulary = 500000):\n",
    "        wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format(gensim_file_path, binary = True, limit = vocabulary)\n",
    "        self.emb_size = 300 # default\n",
    "        self.embeddings = wv_embeddings\n",
    "        self.emb_source = 'load'           \n",
    "    \n",
    "    def _sentence_to_vec(self, sentence_tokenized):\n",
    "        \"\"\"\n",
    "        Takes a mean from words embeddings in sentence\n",
    "        This guarantees fix dim\n",
    "        \"\"\"\n",
    "        words = sentence_tokenized\n",
    "\n",
    "        words_embeddings = [self.embeddings[word] for word in words if word in self.embeddings]\n",
    "        if not words_embeddings:\n",
    "            words_embeddings.append( np.zeros(self.emb_size) )\n",
    "\n",
    "        return np.mean(words_embeddings, axis = 0)\n",
    "\n",
    "    def _sentence_to_word_vec(self, sentence_tokenized):\n",
    "        \"\"\"\n",
    "        Tranform list of words to array of embeddings\n",
    "        For words not in embeddings we set embedding for word 'UNK' (undefined)\n",
    "        \"\"\"\n",
    "        words = sentence_tokenized\n",
    "        \n",
    "        # embedding for undefined word\n",
    "        if self.embeddings.wv.__contains__('UNK'):\n",
    "            UNK = self.embeddings.wv.__getitem__('UNK')\n",
    "        else:\n",
    "            UNK = np.zeros(self.emb_size)\n",
    "            \n",
    "        words_embeddings = [self.embeddings[word] if self.embeddings.wv.__contains__(word) else UNK for word in words]\n",
    "        if not words_embeddings:\n",
    "            words_embeddings.append( np.zeros(self.emb_size) )\n",
    "\n",
    "        return np.array(words_embeddings)\n",
    "    \n",
    "    def fit(self, X = None, y = None,  **fit_params):\n",
    "\n",
    "        if self.embeddings is None:\n",
    "            if self.emb_source == 'load':\n",
    "                self._build_embeddings_gensim_load()\n",
    "            elif self.emb_source == 'own':\n",
    "                self._build_embeddings_own(X, self.emb_size)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\"\n",
    "        X should be tokenized    \n",
    "        X type - list of lists, sieries of lists/strings\n",
    "        \"\"\"        \n",
    "        if self.emb_type == 'avg':\n",
    "            X_trans = np.array( [self._sentence_to_vec(sentence) for sentence in X] )  \n",
    "        elif self.emb_type == 'each':\n",
    "            X_trans = np.array( [self._sentence_to_word_vec(sentence) for sentence in X] ) \n",
    "        else:\n",
    "            print( 'ERROR function make_embedding() - unknown type of: ', emb_type )\n",
    "            \n",
    "        return X_trans\n",
    "            \n",
    "\n",
    "class FixLength(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"standardize length of the sentences\"\"\" \n",
    "    \n",
    "    def __init__(self, max_length = None, fill = '.'):\n",
    "        \"\"\"\n",
    "        max_length (int) - if None, max length will be set to the longest sentance in data set\n",
    "        fill (str) - mark which will be used to fill the missing words up to standardized length \n",
    "            of the sentence (if shorter)\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.fill = fill\n",
    "\n",
    "    def _standardize_len(self, sentence_tokenized ):\n",
    "        sentence_tokenized = sentence_tokenized[0:self.max_length]       \n",
    "        return sentence_tokenized + [self.fill] * (self.max_length - len(sentence_tokenized) )\n",
    "      \n",
    "    def fit(self, X, y = None,  **fit_params):\n",
    "        \"\"\"X should be tokenized\"\"\"\n",
    "        if self.max_length is None:\n",
    "            self.max_length = max(X.apply(len))\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X.apply(self._standardize_len)  \n",
    "    \n",
    "\n",
    "class Flatten(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Flatten the output\"\"\"\n",
    "    def fit(self, X, y = None,  **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return X.reshape(X.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jbi9ozRlTK3"
   },
   "source": [
    "## Tag, SentiFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yo9ZOEEGlTK5",
    "outputId": "181cc1a8-b4ce-46ce-f85d-aef050bf3ac7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classes:\n",
    "    Tag - finds part of the speech for every word in sentence\n",
    "    SentiFeatures - Calculates sentiment for every word in sentence using sentiwordnet\n",
    "\"\"\"\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "class Tag(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Find part of the speech for every word\n",
    "    \n",
    "    Text shouldn't be tokenized, each entry must have fix length, don't use \n",
    "    stopwords removal and stemming before\n",
    "    \"\"\"\n",
    "    def __init__(self, only_tags = False):\n",
    "        self.only_tags = only_tags\n",
    "    \n",
    "    def fit(self, X, y = None,  **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        if self.only_tags:\n",
    "            return  X.apply(pos_tag).apply(lambda tuples_list: [tuples[1] for tuples in tuples_list])\n",
    "        else:\n",
    "            return  X.apply(pos_tag)\n",
    "\n",
    "\n",
    "class SentiFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Calculates sentiment for every word in sentence using sentiwordnet.\n",
    "    Must be applied after Tag transformation (class Tag).\n",
    "        \n",
    "    only_pos_neg - calculate sentiment using only positive and negative feature\n",
    "        (if False, use positive, negative, objective feature)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, only_pos_neg = False):\n",
    "        self.lemmatizer =  WordNetLemmatizer()\n",
    "        self.only_pos_neg = only_pos_neg\n",
    "    \n",
    "    def _penn_to_wn(self, tag):\n",
    "        \"\"\"Convert between the PennTreebank tags to simple Wordnet tags\"\"\"\n",
    "        if tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        elif tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        return None\n",
    "    \n",
    "    def _word_sentiment(self, word, tag):\n",
    "        \"\"\"Calculates sentiments for single word, (pos, neg, obj)\"\"\"\n",
    "        wn_tag = self._penn_to_wn(tag)\n",
    "        default_return = tuple([0.0] * ((not self.only_pos_neg) + 2))\n",
    "        \n",
    "        if not wn_tag:\n",
    "            return default_return\n",
    "\n",
    "        lemma = self.lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            return default_return\n",
    "\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            return default_return\n",
    "        \n",
    "        SentiSynsets_list = list(map( lambda synset: swn.senti_synset(synset.name()), synsets))\n",
    "        \n",
    "        # return (pos, neg, obj) sentiment or just (pos, neg)\n",
    "        if self.only_pos_neg:\n",
    "            SentiSynsets_list = [ (SentiSynsets.pos_score(), SentiSynsets.neg_score()) \n",
    "                                 for SentiSynsets in SentiSynsets_list]            \n",
    "        else:\n",
    "            SentiSynsets_list = [ (SentiSynsets.pos_score(), SentiSynsets.neg_score(), SentiSynsets.obj_score()) \n",
    "                                 for SentiSynsets in SentiSynsets_list]\n",
    "                                \n",
    "        # because one word can have more than one meaning (so multiple sentiments)\n",
    "        # we take average of pos, neg and obj sentiment across all meanings\n",
    "        # return tuple( avg_pos, avg_neg, avg_obj)\n",
    "        return tuple([np.mean(sent) for sent in zip(*SentiSynsets_list)])    \n",
    "    \n",
    "    def sentence_sentiment(self, tagged_sentence):\n",
    "        \"\"\"\n",
    "        Calculates sentiments for each pair (word, tag) in sentence\n",
    "        tagged_sentence - list of tuples (word, tag)\n",
    "        \"\"\"\n",
    "        sentiments_list = [ self._word_sentiment(word, tag) for word, tag in tagged_sentence] \n",
    "              \n",
    "        return np.array(sentiments_list)\n",
    "    \n",
    "    def fit(self, X, y = None,  **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None, **transform_params):\n",
    "        return np.stack( X.apply(self.sentence_sentiment).values ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cROqczX8lTLG"
   },
   "source": [
    "## LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jFzgaA7clTLI"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "class CustomBinarizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    LabelBinarizer has problem to work by its own, \n",
    "    it needed to be wraped to work properly\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None,**fit_params):\n",
    "        self.lb = LabelBinarizer().fit(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return self.lb.transform(X)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "classes.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
